MODEL:
    CONTEXT_ENCODER:
        NAME: MTREncoder

        NUM_OF_ATTN_NEIGHBORS: 16
        NUM_INPUT_ATTR_AGENT: 29
        NUM_INPUT_ATTR_MAP: 9
        
        NUM_CHANNEL_IN_MLP_AGENT: 256
        NUM_CHANNEL_IN_MLP_MAP: 64
        NUM_LAYER_IN_MLP_AGENT: 3
        NUM_LAYER_IN_MLP_MAP: 5
        NUM_LAYER_IN_PRE_MLP_MAP: 3

        D_MODEL: 256
        NUM_ATTN_LAYERS: 6
        NUM_ATTN_HEAD: 8 
        DROPOUT_OF_ATTN: 0.1 

        USE_LOCAL_ATTN: True

    MOTION_DECODER:
        NAME: BCDecoder

        # OBJECT_TYPE: *object_type 
        # CENTER_OFFSET_OF_MAP: *center_offset

        NUM_MOTION_MODES: 6

        D_MODEL: 256
        NUM_DECODER_LAYERS: 4
        NUM_ATTN_HEAD: 8
        MAP_D_MODEL: 256
        DROPOUT_OF_ATTN: 0.1 

        NUM_BASE_MAP_POLYLINES: 256
        NUM_WAYPOINT_MAP_POLYLINES: 128

        LOSS_MODE: 'best'
        USE_BICYCLE_MODEL: False
    
    Q_DECODER:
        NAME: Qecoder

        # OBJECT_TYPE: *object_type 
        # CENTER_OFFSET_OF_MAP: *center_offset

        NUM_Q_MODES: 2

        D_MODEL: 256
        NUM_DECODER_LAYERS: 1
        NUM_ATTN_HEAD: 8
        MAP_D_MODEL: 256
        DROPOUT_OF_ATTN: 0.1 

        NUM_BASE_MAP_POLYLINES: 256
        NUM_WAYPOINT_MAP_POLYLINES: 128


SAC:
    ACTOR:
        ACTOR_TYPE: 'max'
        
        # Optimization
        LR: 0.0001
        LR_STEP_SIZE: 100000
        LR_GAMMA: 0.5
        LR_END: 0.000001

        # Alpha for entropy regularization
        ENTROPY_REG: True
        UPDATE_ALPHA: True
        LOG_ALPHA_INIT: 0
        ALPHA_LR: 0.0001
        ALPHA_LR_PERIOD: 100000
        ALPHA_LR_DECAY: 0.5
        ALPHA_LR_END: 0.000001
        TARGET_ENTROPY: -2.0
    
    CRITIC:
        MODE: 'safety'
        
        # Optimization
        LR: 0.0001
        LR_STEP_SIZE: 100000
        LR_GAMMA: 0.5
        LR_END: 0.000001

        # Alpha for entropy regularization
        ENTROPY_REG: True
        UPDATE_GAMMA: True
        GAMMA: 0.99
        GAMMA_PERIOD: 100000
        GAMMA_DECAY: 0.5
        GAMMA_END: 0.000001

    RL:
        MAX_STEPS: 1_000_000
        OPT_PERIOD: 100
        MIN_STEP_BEFORE_TRAIN: 100
        NUM_UPDATES: 100
        BATCH_SIZE: 8
        EVAL_PERIOD: 10_000
        ACTOR_UPDATE_PERIOD: 1    
        SOFT_UPDATE_PERIOD: 1
        SOFT_UPDATE_TAU: 0.005
        MEMORY_SIZE: 10_000
        OUT_DIR: 'output/rl_finetune'
        USE_WANDB: False
